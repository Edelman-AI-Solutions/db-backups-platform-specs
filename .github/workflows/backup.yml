name: Back up database

on:
  push:
    branches:
      - main
  workflow_dispatch:
    inputs:
      force_deploy:
        description: 'Deploy even if no changes detected'
        required: false 
        type: boolean
  schedule:
    - cron: '0 13 * * 1-5'

jobs:
  backup:
    runs-on: ubuntu-latest
    outputs:
      change_detected: ${{ steps.commit_and_push.outputs.change_detected }}
    steps:
    - name: Check out repo
      uses: actions/checkout@v3
    - name: Set up Python
      uses: actions/setup-python@v3
      with:
        python-version: "3.10"
    - uses: actions/cache@v3
      name: Configure pip caching
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    - name: Install Python dependencies
      run: |
        pip install -r requirements.txt
    - name: Import Postgres Database into SQLite
      env:
        DATABASE_URL: ${{ secrets.DATABASE_URL }}
      run: |-
        db-to-sqlite "$DATABASE_URL" platform_specs.db \
          --table content_item
        sqlite-utils tables platform_specs.db --counts --columns
    - name: Various database modifications
      run: |-
        # Unique index on content_item.slug so Datasette uses it as a foreign key label
        sqlite-utils create-index platform_specs.db content_item slug --unique
    - name: Convert to newline-delimited JSON
      run: |-
        rm platform-specs/* || true
        sqlite-diffable dump platform_specs.db platform_specs --all
    - name: Commit any changes
      id: commit_and_push
      run: |-
        git config user.name "Automated"
        git config user.email "actions@users.noreply.github.com"
        git add platform_specs
        timestamp=$(date -u)
        git commit -m "Latest data: ${timestamp}" || exit 0
        git push
        echo "change_detected=1" >> $GITHUB_OUTPUT
    - name: Install Node
      uses: actions/setup-node@v3
      with:
        node-version: 18
    - name: Install Vercel
      run: npm install -g vercel
    - name: publish to vercel repo
      run: |
        datasette publish vercel platform_specs.db \
          --project db-platform-specs \
          --generate-vercel-json > vercel.json
        sed -i "s/3.0.7/4.5.1/" vercel.json
        datasette publish vercel platform_specs.db \
          --token  ${{ secrets.VERCEL_TOKEN }} \
          --project db-platform-specs \
          --vercel-json=vercel.json \
          --no-prod
  # build_and_deploy:
    # runs-on: ubuntu-latest
    # needs: backup
    # if: >
      # ${{ inputs.force_deploy || needs.backup.outputs.change_detected || 
        # contains(github.event.workflow_run.head_commit.modified, '.github/workflows/backup.yml') }}
    # steps:
  #   - name: Check out repo
  #     uses: actions/checkout@v3
  #     with:
  #       ref: main        
  #   - name: Set up Python
  #     uses: actions/setup-python@v3
  #     with:
  #       python-version: "3.9"
  #   - uses: actions/cache@v3
  #     name: Configure pip caching
  #     with:
  #       path: ~/.cache/pip
  #       key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
  #       restore-keys: |
  #         ${{ runner.os }}-pip-
  #   - name: Install Python dependencies
  #     run: |
  #       pip install -r requirements.txt
        
    # - name: Restore previous database
    #   continue-on-error: true
    #   run: |-
    #     wget -q https://datasette.simonwillison.net/simonwillisonblog.db
  #   - name: Vacuum restored database
  #     run: |-
  #       if [ -f simonwillisonblog.db ]; then
  #         if ! sqlite3 simonwillisonblog.db 'VACUUM;'; then
  #           echo "::warning::sqlite3 VACUUM failed; attempting recovery"
  #           sqlite3 simonwillisonblog.db ".recover" | sqlite3 recovered.db
  #           if sqlite3 recovered.db 'VACUUM;'; then
  #             echo "::notice::Recovery successful, using recovered database"
  #             mv recovered.db simonwillisonblog.db
  #           else
  #             echo "::warning::Recovery failed; deleting simonwillisonblog.db and continuing"
  #             rm -f simonwillisonblog.db recovered.db || true
  #           fi
  #         fi
  #       fi
  #   - name: Build database
  #     run: |-
  #       sqlite-diffable load simonwillisonblog.db simonwillisonblog --replace
  #       sqlite-utils tables simonwillisonblog.db --counts
  #   - name: Extract entities
  #     env:
  #       AWS_ACCESS_KEY_ID: ${{ secrets.COMPREHEND_ACCESS_KEY }}
  #       AWS_SECRET_ACCESS_KEY: ${{ secrets.COMPREHEND_SECRET_KEY }}
  #       AWS_DEFAULT_REGION: us-west-2
  #     run: |-
  #       which sqlite-comprehend
  #       sqlite-comprehend --version
  #       sqlite-comprehend --help
  #       sqlite-comprehend entities --help
  #       sqlite-comprehend entities simonwillisonblog.db blog_entry title body --strip-tags
  #       sqlite-utils tables simonwillisonblog.db --counts
  #   - name: Import the OpenAI embeddings
  #     run: |-
  #       wget https://static.simonwillison.net/static/2023/blog-embeddings/blog.db
  #       # Create table (if it does not exist yet)
  #       sqlite-utils create-table simonwillisonblog.db blog_entry_embeddings \
  #         id integer embedding blob --pk id --ignore
  #       # Import/replace the embeddings
  #       sqlite-utils simonwillisonblog.db --attach embeddings blog.db \
  #         'replace into blog_entry_embeddings select cast(id as integer), embedding from embeddings.embeddings'
  #   - name: Configure FTS
  #     run: |-
  #       set +e
  #       sqlite-utils enable-fts simonwillisonblog.db blog_series title summary --create-triggers --tokenize porter 2>/dev/null
  #       sqlite-utils enable-fts simonwillisonblog.db blog_tag tag --create-triggers --tokenize porter 2>/dev/null
  #       sqlite-utils enable-fts simonwillisonblog.db blog_quotation quotation source --create-triggers --tokenize porter 2>/dev/null
  #       sqlite-utils enable-fts simonwillisonblog.db blog_entry title body --create-triggers --tokenize porter 2>/dev/null
  #       sqlite-utils enable-fts simonwillisonblog.db blog_blogmark link_title via_title commentary --create-triggers --tokenize porter 2>/dev/null
  #       set -e
  #       # Re-populate tables just in case they're missing anything
  #       sqlite-utils populate-fts simonwillisonblog.db blog_series title summary
  #       sqlite-utils populate-fts simonwillisonblog.db blog_tag tag
  #       sqlite-utils populate-fts simonwillisonblog.db blog_quotation quotation source
  #       sqlite-utils populate-fts simonwillisonblog.db blog_entry title body
  #       sqlite-utils populate-fts simonwillisonblog.db blog_blogmark link_title via_title commentary
  #   - name: Copy in latest TILs
  #     run: |
  #       wget https://s3.amazonaws.com/til.simonwillison.net/tils.db
  #       echo "
  #       attach database 'simonwillisonblog.db' as simonwillisonblog;
  #       attach database 'tils.db' as tils;
  #       drop table if exists simonwillisonblog.til;
  #       create table simonwillisonblog.til as select * from tils.til;
  #       " | sqlite3
      # ---# ────────────────────────── [Backup to Vercel] ──────────────────────────
    # - name: Install Node
    #   uses: actions/setup-node@v3
    #   with:
    #     node-version: 18
    # - name: Install Vercel
    #   run: npm install -g vercel
    # - name: publish to vercel repo
    #   run: |
    #     datasette publish vercel platform_specs.db \
    #     --token  ${{ secrets.VERCEL_TOKEN }} \
    #     --project db-platform-specs
  #   # ---# ────────────────────────── [------------------- ──────────────────────────    
